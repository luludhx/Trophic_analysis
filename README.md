# Improving neural network arichtecture with trophic coherence
A neural networks is a mathematical structure used to model pairwise relations between objects. It is made up of vertices (also called nodes) which are connected by edges. A distinction is made between undirected networks, where edges link two vertices symmetrically, and directed networks, where edges link two vertices asymmetrically. Biological neural networks were first invented in the 1950s to model the brain's neurons (nodes) and synapses (edges). Hebb proposed that synaptic strengths increase if two neurons fire simultaneously (“cells that fire together, wire together”), which served as an early learning principle.

Soon after came artificial neural networks, with the introduction of the Perceptron by Rosenblatt. However, the Perceptron, a simple feedforward neural network with a learning rule that could classify linearly separable data, was quickly dismissed because of its single-layer limitation. In the 1980s neural networks were revitalized by showing that multi-layer Perceptrons (MLPs) could indeed learn complex, non-linear decision boundaries given sufficient data and computational power. Even so, neural networks were not widely used for machine learning, and other methods (like SVMs and decision trees) were the focus. It was only in the early 2000s, with the rise of deep learning and its major breakthroughs in image and speech recognition, reinforcement learning, etc., that artificial neural networks really gained traction.

Deep networks are made up of distinct layers of nodes and are completely feedforward. This structure makes them very efficient at certain tasks like pattern recognition, but they lack any capacity for memory. Such networks (like CNNs) use backpropagation to train, but this is done by minimising a loss function (that quantifies the discrepancy between the model's prediction and the actual target values). There is no integrated mechanism to maintain an internal state once the forward pass is done. On the other hand, very dense networks (like the Hopfield network) have feedback loops in their architecture which allow for backpropagation. They are used in situations where memory, context, or iterative updating of information (such as unrolled RNNs) is important.

The brain network combines feedforward and feedback processing. When we perceive our surroundings, our brain engages in feedforward processing to recognise visual stimuli, allowing us to identify objects and patterns. Simultaneously, feedback processing integrates information from memory and prior experiences. It then seems natural to expect the network of a brain to have an architecture that balances a distinct edge direction with loops and interconnected components.

To quantify this, we use trophic analysis. This technique, inspired by ecological networks, assigns to each node a trophic level. This is similar to a food chain where plants sit at the bottom of the network hierarchy, and the highest trophic level nodes are carnivores at the top of the food chain. The trophic coherence of the whole network is then measured, a property which indicates to what extent the level hierarchy is well defined.
A network is considered trophically coherent when nodes predominantly interact with others at adjacent trophic levels, minimising feedback loops and creating a clear flow of energy.

Trophic structure applies to the broader field of complex networks and has been used to study spreading processes in neural and epidemiological settings, infrastructure, and the structure of organisations. Here, we focus on the trophic structure of neural networks and on the biological networks of different animals' nervous systems. In particular, the neural network of the nematode 	extit{C. elegans}, the only animal nervous system to have been fully mapped at the level of neurons and synapses, has been found to have intermediate trophic coherence. What could account for this specific neural network architecture? We address this question by investigating the connection between trophic structure and the dynamics of a Hopfield-like neural network.
 
